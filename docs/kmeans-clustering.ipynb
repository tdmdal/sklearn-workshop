{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22e651e",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Clustering is a type of __unsupervised machine learning__, where different data points are grouped together into two or more clusters. Data points in the same cluster are more similar to each other than those in other clusters. This __similarity__ can be measured in some specified way and the strength of similarity between data points is used to assign data points to its cluster. \n",
    "\n",
    "There are __hard clustering__ and __soft clustering__ methods. Hard clustering is when each data point belongs to a cluster completely. Soft clustering is when each data point can belong to more than one cluster with some probability. The number of clusters can be defined by the user. However, in some cases even the users do not know how many clusters should the data be grouped into. Therefore, figuring out the best number of cluster is also a part of the clustering task.  \n",
    "\n",
    "### K-Means Clustering\n",
    "* Hard clustering method.\n",
    "* A centroid-based clustering method. \n",
    "* Given a cluster, a __centroid__ is its central data point. \n",
    "* Centroid can be real of imaginary. \n",
    "* In K-Means an iterative algorithm is employed to derive similarity based on the distance of that data point from the centroid of the cluster. \n",
    "\n",
    "Let's begin by downloading a small sample (version 2) of marketing campaign dataset of a Portugese banking institution available on [OpenML](https://www.openml.org/). The data is related to direct marketing campaigns via phone calls to subscribe clients to a bank term deposit. Detailed description of the dataset is available [here](https://www.openml.org/d/1461). \n",
    "\n",
    "We will use the `fetch_openml` function from `datasets` module of sklearn. The function provides easy access to the OpenML API to download available datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d87d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# fetch by using data name and version\n",
    "bank_marketing = fetch_openml(name='bank-marketing', version=2) # try version 1\n",
    "\n",
    "data = bank_marketing.data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190430e3",
   "metadata": {},
   "source": [
    "Let's take a quick look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3ee8e",
   "metadata": {},
   "source": [
    "Each row belongs to a bank client. Each column provides additional details related to that client including data from the last contact of the current campaign. We do not have column names to identify them easily. So let's add them to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d2242",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['age', 'job', 'marital_status', 'education', 'credit_default', 'balance', 'housing', 'loan', \n",
    "             'lastcontact_type', 'lastcontact_dayofmonth', 'lastcontact_month', 'lastcontact_duration', \n",
    "             'n_contacts', 'days_since_lastcontact', 'previous_n_contacts', 'previous_outcome']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a929d85",
   "metadata": {},
   "source": [
    "Our task is to figure out how to devise a marketing campaign to optimize client subscription to bank term deposit. \n",
    "\n",
    "The data is not ready to feed into a ML algorithm. Therefore, we need to perform various __data cleaning__ or __data pre-processing__ steps first. Let's start by creating the feature matrix using select columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d165715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f646c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['job', 'education',  'credit_default', 'balance', 'housing', 'loan']]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a07f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9e1b93",
   "metadata": {},
   "source": [
    "There are no missing values in this datasets. However, there are many `categorical` data types. These categorical data need to be converted into numeric so that ML algorithm implementation tools can accept them. \n",
    "\n",
    "Note that some of these categorical columns have \"unknown\" values and could be treated the same way as missing values. \n",
    "\n",
    "For now, we will use all categories available including \"unknown\". However, I encourage you to look at these categories closely and find new ways of dealing with them such that model performance can perhaps be improved.\n",
    "\n",
    "To convert categorical data into numeric we can use the [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) object from the `preprocessing` module of sklearn. The function follows a `fit_transform` framework used in many other sklearn objects. Given a dataset, the encoder finds the unique categories for each feature and transforms them into a new column, where a value of 1 is given if the row belongs to that category or 0 otherwise. This process is also known as __vector representation__.  \n",
    "\n",
    "__Note__ that `pandas` also offers a method called `get_dummies`, which converts categorical variables into dummy variables much the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b5e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac56321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of categorical columns and isolate these features\n",
    "cat_feat = ['job', 'education', 'credit_default', 'housing', 'loan']\n",
    "X_cat = X[cat_feat]\n",
    "X_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and instance of OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "# apply fit_transform on dataframe with categorical features only \n",
    "X_cat_ohe = enc.fit_transform(X_cat)\n",
    "\n",
    "# convert result into numpy array\n",
    "X_cat_ohe = X_cat_ohe.toarray()\n",
    "\n",
    "# convert result into pandas dataframe\n",
    "X_cat_ohe = pd.DataFrame(X_cat_ohe)\n",
    "X_cat_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5777d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "X_cat_ohe.columns = enc.get_feature_names(cat_feat)\n",
    "X_cat_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa20a748",
   "metadata": {},
   "source": [
    "The categorical data is now in an acceptable format. Let's drop the orginal columns and add these columns instead in the feature matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(cat_feat, axis=1, inplace=True)\n",
    "X = pd.concat([X, X_cat_ohe], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230ab86",
   "metadata": {},
   "source": [
    "Now we can use the `.fit` framework of sklearn to implement the [K-Means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) clustering algorithm in this data. This framework will train the model using the provided data and then obtain subsequent predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f94589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters=2, init='random', random_state=0)\n",
    "k_means.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9802bb61",
   "metadata": {},
   "source": [
    "The parameter `n_cluster` takes the value of the number of clusters we wish to have. Here we have asked the data to be grouped into two clusters. The parameter `init` refers to the method to be used for initialization. We also specify the `random_state` parameter to replicate the result during future runs. We also have a choice of selecting either Lloyd's or Elkan's algorithm. \n",
    "\n",
    "The `KMeans` object has attributes such as `cluster_centers_`, `labels_`, `inertia_` and `n_iter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55aa7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k_means.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k_means.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(k_means.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f93a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k_means.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a064a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k_means.n_iter_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee31d6",
   "metadata": {},
   "source": [
    "### Steps in K-Means Algorithm\n",
    "\n",
    "1. Choose the number of clusters *k*\n",
    "2. Randomly initialize *k* centroids\n",
    "3. Assign each point to its closest centroid\n",
    "4. Compute mean of each cluster and call it the new centroid\n",
    "5. Repeat steps 3 and 4 until the centroid positions do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_3k = KMeans(n_clusters=3, init='random', random_state=0)\n",
    "k_means_3k.fit(X)\n",
    "print(k_means_3k.cluster_centers_)\n",
    "print(k_means_3k.inertia_)\n",
    "print(k_means_3k.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee579447",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k_means_3k.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(k_means_3k.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be3916",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "#### Silhouette coefficient \n",
    "* A measure of cluster cohesion and separation. \n",
    "* Quantifies how well a data point fits into its assigned cluster based on two factors:\n",
    "    * How close the data point is to other points in the __same__ cluster\n",
    "    * How far away the data point is from points in __other__ clusters\n",
    "* Values range between -1 and 1; larger numbers indicate that samples are closer to their assigned clusters than they are to other clusters.\n",
    "\n",
    "[`Silhouette_score` function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) is available in sklearn's `metric` module.\n",
    "\n",
    "#### Elbow Method\n",
    "* A technique to evaluate the best number of cluster *k*.\n",
    "* Run K-Means on same data with multiple values of *k* and choose *k* that minimized the squared sum of errors (`.interia_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70885c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcbd87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = []\n",
    "inertia = []\n",
    "krange = range(2, 11)\n",
    "for i in krange:\n",
    "    print(i)\n",
    "    kmeans = KMeans(n_clusters=i, init='random', random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    sscore = round(silhouette_score(X, kmeans.labels_),2)\n",
    "    silhouette.append(sscore)\n",
    "    inertia.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0371cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.plot(krange, inertia, marker='o')\n",
    "for i, txt in enumerate(silhouette):\n",
    "    plt.annotate('S='+str(txt), (krange[i], inertia[i]))\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of Squared Errors')\n",
    "plt.title('Elbow method for optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9bf43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
